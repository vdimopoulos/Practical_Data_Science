{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19db1bd6",
   "metadata": {},
   "source": [
    "# C. Clustering-based augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_df2.csv\")\n",
    "train_df.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87071282",
   "metadata": {},
   "source": [
    "## Use K-Means, based on an approprate text representation and the (estimated) optimum K, to cluster the generated essays, and then the student essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43abd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = train_df[train_df['generated']==1]\n",
    "student_df = train_df[train_df['generated']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af730dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df['tokenized_text'] = generated_df['text'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.lower() not in ENGLISH_STOP_WORDS])\n",
    "model1 = Word2Vec(sentences=generated_df['tokenized_text'],epochs=10,\n",
    "                                vector_size=300, \n",
    "                                window=3,\n",
    "                                sg=0,\n",
    "                                min_count=2,\n",
    "                                workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47705eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df['tokenized_text'] = student_df['text'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.lower() not in ENGLISH_STOP_WORDS])\n",
    "model2 = Word2Vec(sentences=student_df['tokenized_text'],epochs=10,\n",
    "                                vector_size=300, \n",
    "                                window=3,\n",
    "                                sg=0,\n",
    "                                min_count=2,\n",
    "                                workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_text, model, vocabulary, num_features)\n",
    "                for tokenized_text in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def get_top_words_for_cluster(kmeans_model, vectorizer_model, num_words=10):\n",
    "    order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    feature_names = vectorizer_model.wv.index_to_key\n",
    "    top_words_per_cluster = {}\n",
    "    for i in range(kmeans_model.n_clusters):\n",
    "        top_words = [feature_names[ind] for ind in order_centroids[i, :num_words]]\n",
    "        top_words_per_cluster[f'Cluster {i}'] = top_words\n",
    "    return top_words_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_features = averaged_word_vectorizer(corpus=generated_df['tokenized_text'], model=model1, num_features=300)\n",
    "\n",
    "max_clusters = 10\n",
    "silhouette_scores = []\n",
    "\n",
    "for num_clusters in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(word2vec_features)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(word2vec_features, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from 2 clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)\n",
    "kmeans.fit_predict(word2vec_features)\n",
    "generated_df['cluster'] = kmeans.predict(word2vec_features)\n",
    "\n",
    "top_words_per_cluster = get_top_words_for_cluster(kmeans, model1, num_words=20)\n",
    "\n",
    "for cluster, top_words in top_words_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130aa99",
   "metadata": {},
   "source": [
    "## Yield a title per cluster, reflecting the topic of the texts included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d79ef",
   "metadata": {},
   "source": [
    "Cluster 0: \"Critical Infrastructure Protection\"\n",
    "\n",
    "Cluster 1: \"Air Quality and Environmental Perspectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b598d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_features = averaged_word_vectorizer(corpus=student_df['tokenized_text'], model=model1, num_features=300)\n",
    "\n",
    "# Choose the optimal number of clusters using silhouette score\n",
    "max_clusters = 10\n",
    "silhouette_scores = []\n",
    "\n",
    "for num_clusters in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(word2vec_features)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(word2vec_features, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from 2 clusters\n",
    "\n",
    "# Perform K-means clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)\n",
    "kmeans.fit_predict(word2vec_features)\n",
    "student_df['cluster'] = kmeans.predict(word2vec_features)\n",
    "\n",
    "# Get the top words for each cluster\n",
    "top_words_per_cluster = get_top_words_for_cluster(kmeans, model2, num_words=20)\n",
    "\n",
    "# Display the top words for each cluster\n",
    "for cluster, top_words in top_words_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc544b",
   "metadata": {},
   "source": [
    "Cluster 0: \"Presidential Campaigns and Strategies\"\n",
    "\n",
    "Cluster 1: \"Diverse Opinions on Presidential Driving Factors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3f4a4",
   "metadata": {},
   "source": [
    "## Compare the cluster balance (number of instances per cluster) between the two clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eceef",
   "metadata": {},
   "source": [
    "## Generate more texts (as in A) in order to better balance your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_cluster_1_prompt = 'Write an essay, up to 600 words with topic: \"Diverse Opinions on Presidential Driving Factors\". Similar essays had as top 20 words: smaller, presidential, duffer, united, driving, does, away, popular, count, 4, reasons, congress, free, walter, government, didnt, bogota, little, process, number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_cluster_0_prompt = 'Write an essay, up to 600 words with topic:\"Critical Infrastructure Protection\". Similar essays had as top 20 words:safeguard, popular, numerous, infrastructure, voices, heavily, matter, spaces, critics, thank, outcomes, intro, activity, imperative, economic, transit, promote, representative, inclusive, prevents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0018e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from openai import OpenAI\n",
    "import json\n",
    "essays_list = []\n",
    "client = OpenAI(api_key = \"\")\n",
    "\n",
    "for i in range(40):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": student_cluster_1_prompt}\n",
    "      ]\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    essays_list.append(text)\n",
    "for i in range(170):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": generated_cluster_0_prompt}\n",
    "      ]\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    essays_list.append(text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25146a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import re\n",
    "def remove_punctuation_except_periods(text):\n",
    "    return re.sub(r'[^\\w\\s.]', '', text)\n",
    "\n",
    "def remove_specific_words(text):\n",
    "    words_to_remove = ['essay', 'body', 'introduction','conclusion','title','response',' \\n'] + [f'paragraph{x}' for x in range(10)]  # Adjust the range as needed\n",
    "    pattern = '\\\\b(?:' + '|'.join(map(re.escape, words_to_remove)) + ')\\\\b'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_punctuation_except_periods(text)\n",
    "    text = remove_specific_words(text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb337f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_df1 = pd.read_csv('train_df.csv')\n",
    "train_df2 = pd.read_csv('train_df2.csv')\n",
    "train_df1.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df2.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df1 = train_df1[train_df1['generated']==1]\n",
    "train_df2 = train_df2[train_df2['generated']==1]\n",
    "augmentation = pd.concat([train_df1,train_df2])\n",
    "for i in range(210):\n",
    "    essays_list[i] = clean_text(essays_list[i])\n",
    "    if (i < 40):\n",
    "        new_row = {'prompt_id':3, 'text':essays_list[i],'generated':1}\n",
    "    else:\n",
    "        new_row = {'prompt_id':4, 'text':essays_list[i],'generated':1}\n",
    "    augmentation = augmentation.append(new_row, ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation.to_csv('augmentation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_df = train_df.append(augmentation[(augmentation['prompt_id']==3)|(augmentation['prompt_id']==4)])\n",
    "train_df.to_csv('train_df_after_C.csv',index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cedc24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filled = pd.read_csv('train_df_after_C.csv')\n",
    "train_df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13111c56",
   "metadata": {},
   "source": [
    "## Re-train your best-performant classifier on the new data (or a careful selection of them) and analyze the benefits of using clustering to improve the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df_filled.text\n",
    "y = train_df_filled.generated\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "X_train_array = X_train_tfidf.toarray()\n",
    "X_test_array = X_test_tfidf.toarray()\n",
    "    \n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train_array, y_train)\n",
    "    \n",
    "y_pred = svm_model.predict(X_test_array)\n",
    "accuracy = accuracy_score(y_test, y_pred)  \n",
    "f1 = f1_score(y_test, y_pred) \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715508d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
