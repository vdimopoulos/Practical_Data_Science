{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19db1bd6",
   "metadata": {},
   "source": [
    "# C. Clustering-based augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344b4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vassi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1682b6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Americas love affair with its vehicles seems t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car Do you drive a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>1</td>\n",
       "      <td>letter_to_state_senator \\n recipient Senator L...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage offers numerous advantages ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>1</td>\n",
       "      <td>letter \\n recipient State Senator\\n address St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>0</td>\n",
       "      <td>Due to the character limit I cannot provide a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>1</td>\n",
       "      <td>letter_to_state_senator \\n subject Argument in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2634 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt_id                                               text  generated\n",
       "0             0  Cars. Cars have been around since they became ...          0\n",
       "1             0  Transportation is a large necessity in most co...          0\n",
       "2             0  Americas love affair with its vehicles seems t...          0\n",
       "3             0  How often do you ride in a car Do you drive a ...          0\n",
       "4             0  Cars are a wonderful thing. They are perhaps o...          0\n",
       "...         ...                                                ...        ...\n",
       "2629          1  letter_to_state_senator \\n recipient Senator L...          1\n",
       "2630          0  Limiting car usage offers numerous advantages ...          1\n",
       "2631          1  letter \\n recipient State Senator\\n address St...          1\n",
       "2632          0  Due to the character limit I cannot provide a ...          1\n",
       "2633          1  letter_to_state_senator \\n subject Argument in...          1\n",
       "\n",
       "[2634 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train_df2.csv\")\n",
    "train_df.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87071282",
   "metadata": {},
   "source": [
    "## Use K-Means, based on an approprate text representation and the (estimated) optimum K, to cluster the generated essays, and then the student essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a43abd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df = train_df[train_df['generated']==1]\n",
    "student_df = train_df[train_df['generated']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af730dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_df['tokenized_text'] = generated_df['text'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.lower() not in ENGLISH_STOP_WORDS])\n",
    "model1 = Word2Vec(sentences=generated_df['tokenized_text'],epochs=10,\n",
    "                                vector_size=300, \n",
    "                                window=3,\n",
    "                                sg=0,\n",
    "                                min_count=2,\n",
    "                                workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47705eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df['tokenized_text'] = student_df['text'].apply(lambda x: [word.lower() for word in word_tokenize(x) if word.lower() not in ENGLISH_STOP_WORDS])\n",
    "model2 = Word2Vec(sentences=student_df['tokenized_text'],epochs=10,\n",
    "                                vector_size=300, \n",
    "                                window=3,\n",
    "                                sg=0,\n",
    "                                min_count=2,\n",
    "                                workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db24c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_text, model, vocabulary, num_features)\n",
    "                for tokenized_text in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "def get_top_words_for_cluster(kmeans_model, vectorizer_model, num_words=10):\n",
    "    order_centroids = kmeans_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    feature_names = vectorizer_model.wv.index_to_key\n",
    "    top_words_per_cluster = {}\n",
    "    for i in range(kmeans_model.n_clusters):\n",
    "        top_words = [feature_names[ind] for ind in order_centroids[i, :num_words]]\n",
    "        top_words_per_cluster[f'Cluster {i}'] = top_words\n",
    "    return top_words_per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c5cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Cluster 0: heavily, numerous, popular, safeguard, spaces, infrastructure, thank, matter, imperative, critics, livable, intro, voices, ensure, ultimately, inclusive, activity, representative, outcomes, transit\n",
      "Cluster Cluster 1: dominance, electing, including, promoting, air, large, perspectives, component, counterclaim, better, additionally, citizens, emissions, stability, pollution, positive, reducing, carbon, furthermore, implications\n"
     ]
    }
   ],
   "source": [
    "word2vec_features = averaged_word_vectorizer(corpus=generated_df['tokenized_text'], model=model1, num_features=300)\n",
    "\n",
    "max_clusters = 10\n",
    "silhouette_scores = []\n",
    "\n",
    "for num_clusters in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(word2vec_features)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(word2vec_features, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from 2 clusters\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)\n",
    "kmeans.fit_predict(word2vec_features)\n",
    "generated_df['cluster'] = kmeans.predict(word2vec_features)\n",
    "\n",
    "top_words_per_cluster = get_top_words_for_cluster(kmeans, model1, num_words=20)\n",
    "\n",
    "for cluster, top_words in top_words_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c130aa99",
   "metadata": {},
   "source": [
    "## Yield a title per cluster, reflecting the topic of the texts included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d79ef",
   "metadata": {},
   "source": [
    "Cluster 0: \"Critical Infrastructure Protection\"\n",
    "\n",
    "Cluster 1: \"Air Quality and Environmental Perspectives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b598d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Cluster 0: stated, fewer, idea, presidency, walk, says, example, americans, presidential, alternative, means, 12, different, smaller, benefit, happen, chance, posner, healthier, said\n",
      "Cluster Cluster 1: presidential, united, duffer, government, smaller, driving, 4, colombia, popular, away, free, does, reasons, congress, reason, didnt, little, party, walter, bogota\n"
     ]
    }
   ],
   "source": [
    "word2vec_features = averaged_word_vectorizer(corpus=student_df['tokenized_text'], model=model1, num_features=300)\n",
    "\n",
    "# Choose the optimal number of clusters using silhouette score\n",
    "max_clusters = 10\n",
    "silhouette_scores = []\n",
    "\n",
    "for num_clusters in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(word2vec_features)\n",
    "    labels = kmeans.labels_\n",
    "    silhouette_avg = silhouette_score(word2vec_features, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started from 2 clusters\n",
    "\n",
    "# Perform K-means clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_num_clusters, random_state=42)\n",
    "kmeans.fit_predict(word2vec_features)\n",
    "student_df['cluster'] = kmeans.predict(word2vec_features)\n",
    "\n",
    "# Get the top words for each cluster\n",
    "top_words_per_cluster = get_top_words_for_cluster(kmeans, model2, num_words=20)\n",
    "\n",
    "# Display the top words for each cluster\n",
    "for cluster, top_words in top_words_per_cluster.items():\n",
    "    print(f\"Cluster {cluster}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc544b",
   "metadata": {},
   "source": [
    "Cluster 0: \"Presidential Campaigns and Strategies\"\n",
    "\n",
    "Cluster 1: \"Diverse Opinions on Presidential Driving Factors\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3f4a4",
   "metadata": {},
   "source": [
    "## Compare the cluster balance (number of instances per cluster) between the two clusterings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca4ee12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    706\n",
       "1    666\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4fdf9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    716\n",
       "0    546\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3eceef",
   "metadata": {},
   "source": [
    "## Generate more texts (as in A) in order to better balance your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dffa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_cluster_1_prompt = 'Write an essay, up to 600 words with topic: \"Diverse Opinions on Presidential Driving Factors\". Similar essays had as top 20 words: smaller, presidential, duffer, united, driving, does, away, popular, count, 4, reasons, congress, free, walter, government, didnt, bogota, little, process, number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8672f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_cluster_0_prompt = 'Write an essay, up to 600 words with topic:\"Critical Infrastructure Protection\". Similar essays had as top 20 words:safeguard, popular, numerous, infrastructure, voices, heavily, matter, spaces, critics, thank, outcomes, intro, activity, imperative, economic, transit, promote, representative, inclusive, prevents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0018e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from openai import OpenAI\\nimport json\\nessays_list = []\\nclient = OpenAI(api_key = \"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\")\\n\\nfor i in range(40):\\n    response = client.chat.completions.create(\\n      model=\"gpt-3.5-turbo-1106\",\\n      response_format={ \"type\": \"json_object\" },\\n      messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\\n        {\"role\": \"user\", \"content\": student_cluster_1_prompt}\\n      ]\\n    )\\n    text = response.choices[0].message.content\\n    essays_list.append(text)\\nfor i in range(170):\\n    response = client.chat.completions.create(\\n      model=\"gpt-3.5-turbo-1106\",\\n      response_format={ \"type\": \"json_object\" },\\n      messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\\n        {\"role\": \"user\", \"content\": generated_cluster_0_prompt}\\n      ]\\n    )\\n    text = response.choices[0].message.content\\n    essays_list.append(text)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from openai import OpenAI\n",
    "import json\n",
    "essays_list = []\n",
    "client = OpenAI(api_key = \"\")\n",
    "\n",
    "for i in range(40):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": student_cluster_1_prompt}\n",
    "      ]\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    essays_list.append(text)\n",
    "for i in range(170):\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-1106\",\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": generated_cluster_0_prompt}\n",
    "      ]\n",
    "    )\n",
    "    text = response.choices[0].message.content\n",
    "    essays_list.append(text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25146a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import re\\ndef remove_punctuation_except_periods(text):\\n    return re.sub(r'[^\\\\w\\\\s.]', '', text)\\n\\ndef remove_specific_words(text):\\n    words_to_remove = ['essay', 'body', 'introduction','conclusion','title','response',' \\n'] + [f'paragraph{x}' for x in range(10)]  # Adjust the range as needed\\n    pattern = '\\\\b(?:' + '|'.join(map(re.escape, words_to_remove)) + ')\\\\b'\\n    return re.sub(pattern, '', text, flags=re.IGNORECASE)\\n\\ndef clean_text(text):\\n    text = remove_punctuation_except_periods(text)\\n    text = remove_specific_words(text)\\n    text = re.sub(r' +', ' ', text)\\n    text = text.strip()\\n    return text\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import re\n",
    "def remove_punctuation_except_periods(text):\n",
    "    return re.sub(r'[^\\w\\s.]', '', text)\n",
    "\n",
    "def remove_specific_words(text):\n",
    "    words_to_remove = ['essay', 'body', 'introduction','conclusion','title','response',' \\n'] + [f'paragraph{x}' for x in range(10)]  # Adjust the range as needed\n",
    "    pattern = '\\\\b(?:' + '|'.join(map(re.escape, words_to_remove)) + ')\\\\b'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_punctuation_except_periods(text)\n",
    "    text = remove_specific_words(text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb337f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_df1 = pd.read_csv('train_df.csv')\\ntrain_df2 = pd.read_csv('train_df2.csv')\\ntrain_df1.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\\ntrain_df2.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\\ntrain_df1 = train_df1[train_df1['generated']==1]\\ntrain_df2 = train_df2[train_df2['generated']==1]\\naugmentation = pd.concat([train_df1,train_df2])\\nfor i in range(210):\\n    essays_list[i] = clean_text(essays_list[i])\\n    if (i < 40):\\n        new_row = {'prompt_id':3, 'text':essays_list[i],'generated':1}\\n    else:\\n        new_row = {'prompt_id':4, 'text':essays_list[i],'generated':1}\\n    augmentation = augmentation.append(new_row, ignore_index=True)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_df1 = pd.read_csv('train_df.csv')\n",
    "train_df2 = pd.read_csv('train_df2.csv')\n",
    "train_df1.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df2.drop(axis=1,columns=['Unnamed: 0'],inplace=True)\n",
    "train_df1 = train_df1[train_df1['generated']==1]\n",
    "train_df2 = train_df2[train_df2['generated']==1]\n",
    "augmentation = pd.concat([train_df1,train_df2])\n",
    "for i in range(210):\n",
    "    essays_list[i] = clean_text(essays_list[i])\n",
    "    if (i < 40):\n",
    "        new_row = {'prompt_id':3, 'text':essays_list[i],'generated':1}\n",
    "    else:\n",
    "        new_row = {'prompt_id':4, 'text':essays_list[i],'generated':1}\n",
    "    augmentation = augmentation.append(new_row, ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb3eda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation.to_csv('augmentation.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38dfd9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_df = train_df.append(augmentation[(augmentation['prompt_id']==3)|(augmentation['prompt_id']==4)])\\ntrain_df.to_csv('train_df_after_C.csv',index=False)\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_df = train_df.append(augmentation[(augmentation['prompt_id']==3)|(augmentation['prompt_id']==4)])\n",
    "train_df.to_csv('train_df_after_C.csv',index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cedc24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Americas love affair with its vehicles seems t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car Do you drive a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>4</td>\n",
       "      <td>Critical Infrastructure Protection\\n word_coun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>4</td>\n",
       "      <td>Critical Infrastructure Protection\\n word_coun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>4</td>\n",
       "      <td>Critical Infrastructure Protection is an imper...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>4</td>\n",
       "      <td>Critical Infrastructure Protection\\n word_coun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>4</td>\n",
       "      <td>Critical Infrastructure Protection\\n word_coun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3054 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt_id                                               text  generated\n",
       "0             0  Cars. Cars have been around since they became ...          0\n",
       "1             0  Transportation is a large necessity in most co...          0\n",
       "2             0  Americas love affair with its vehicles seems t...          0\n",
       "3             0  How often do you ride in a car Do you drive a ...          0\n",
       "4             0  Cars are a wonderful thing. They are perhaps o...          0\n",
       "...         ...                                                ...        ...\n",
       "3049          4  Critical Infrastructure Protection\\n word_coun...          1\n",
       "3050          4  Critical Infrastructure Protection\\n word_coun...          1\n",
       "3051          4  Critical Infrastructure Protection is an imper...          1\n",
       "3052          4  Critical Infrastructure Protection\\n word_coun...          1\n",
       "3053          4  Critical Infrastructure Protection\\n word_coun...          1\n",
       "\n",
       "[3054 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_filled = pd.read_csv('train_df_after_C.csv')\n",
    "train_df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13111c56",
   "metadata": {},
   "source": [
    "## Re-train your best-performant classifier on the new data (or a careful selection of them) and analyze the benefits of using clustering to improve the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "381f3cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983633387888707\n",
      "F1 Score: 0.9984871406959154\n"
     ]
    }
   ],
   "source": [
    "X = train_df_filled.text\n",
    "y = train_df_filled.generated\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "X_train_array = X_train_tfidf.toarray()\n",
    "X_test_array = X_test_tfidf.toarray()\n",
    "    \n",
    "svm_model = LinearSVC(random_state=42)\n",
    "svm_model.fit(X_train_array, y_train)\n",
    "    \n",
    "y_pred = svm_model.predict(X_test_array)\n",
    "accuracy = accuracy_score(y_test, y_pred)  \n",
    "f1 = f1_score(y_test, y_pred) \n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715508d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
